{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating JSON file for our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#  Define input & output file paths\n",
    "input_file = r\"C:\\Users\\gvais\\OneDrive\\Desktop\\faq\\faq text.txt\"\n",
    "output_file = r\"C:\\Users\\gvais\\OneDrive\\Desktop\\faq\\faq_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet  # Detects file encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store extracted FAQs\n",
    "faqs = []\n",
    "question = None\n",
    "answer = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Detected Encoding: Windows-1252\n"
     ]
    }
   ],
   "source": [
    "# Detect File Encoding First (To Avoid Unicode Errors)\n",
    "with open(input_file, \"rb\") as f:\n",
    "    raw_data = f.read(50000)  # Read a portion of the file\n",
    "    result = chardet.detect(raw_data)\n",
    "    detected_encoding = result[\"encoding\"]\n",
    "\n",
    "print(f\" Detected Encoding: {detected_encoding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Read the text file with detected encoding (Avoid Unicode Errors)\n",
    "with open(input_file, \"r\", encoding=detected_encoding, errors=\"ignore\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Try multiple encodings (utf-8-sig, latin-1, or errors=\"replace\" to avoid crashes)\n",
    "with open(input_file, \"r\", encoding=\"utf-8-sig\", errors=\"replace\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "\n",
    "        if line.startswith(\"Q-\"):  # Identify questions\n",
    "            if question and answer:  # Store previous Q&A before starting a new one\n",
    "                faqs.append({\"question\": question, \"answer\": answer.strip()})\n",
    "            question = line[2:].strip()  # Remove 'Q-' and store question\n",
    "            answer = \"\"  # Reset answer for new question\n",
    "\n",
    "        elif line.startswith(\"A-\"):  # Identify answers\n",
    "            answer = line[2:].strip()  # Remove 'A-' and store answer\n",
    "\n",
    "        else:  # Append multi-line answers\n",
    "            answer += \" \" + line\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FAQs successfully extracted and saved to: C:\\Users\\gvais\\OneDrive\\Desktop\\faq\\faq_data.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Append the last Q&A pair\n",
    "if question and answer:\n",
    "    faqs.append({\"question\": question, \"answer\": answer.strip()})\n",
    "\n",
    "# Save FAQs to JSON file\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump({\"faqs\": faqs}, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\" FAQs successfully extracted and saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "faq_file = r\"C:\\Users\\gvais\\OneDrive\\Desktop\\faq\\faq_data.json\"  # Update path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load JSON file correctly\n",
    "with open(faq_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    faqs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of faqs: <class 'dict'>\n",
      "Dictionary keys: dict_keys(['faqs'])\n"
     ]
    }
   ],
   "source": [
    "#  Check the type of `faqs`\n",
    "print(\"Type of faqs:\", type(faqs))  # Should be <class 'list'>\n",
    "\n",
    "#  If it's a dictionary, print keys\n",
    "if isinstance(faqs, dict):\n",
    "    print(\"Dictionary keys:\", faqs.keys())\n",
    "\n",
    "#  If it's a list, print first 3 entries\n",
    "if isinstance(faqs, list):\n",
    "    print(faqs[:3])  # Should show a list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gvais\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  1724,   338,   350,  8222,   271, 15666, 29973,   350,  8222,\n",
      "           271, 15666,   338,   263, 18161, 15483,  5001,   856]])\n"
     ]
    }
   ],
   "source": [
    "#loading llama's pretrained tokenizer\n",
    "from transformers import LlamaTokenizer\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "text = \"What is BharatPe? BharatPe is a financial technology company...\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "print(tokens.input_ids)  # Tokenized representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load your FAQs\n",
    "with open(r\"C:\\Users\\gvais\\OneDrive\\Desktop\\faq\\bharatpe_faqs.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    faqs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.48.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load JSON file correctly\n",
    "with open(faq_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    faqs_dict = json.load(f)  # This loads a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of faqs after extraction: <class 'list'>\n",
      "[{'question': 'what is bharatpe?', 'answer': \"bharatpe is a new way to pay, and makes spending easy. in collaboration with lending partners, you can spend now and pay next month. a line of credit is offered through bharatpe by rbi registered lenders (after necessary checks and kyc completion) which can be used anytime, anywhere to purchase goods and services - either by scanning a qr, sending money directly to your bank account, making utility bill payments or availing a personal loan from the assigned bharatpe limit. when it's time to pay your bill, you can clear your dues in one go, or convert to an equated monthly installment (emi) with tenure of your choice. you can also earn exciting cashback and rewards. get started now!\"}, {'question': 'how do i sign up for bharatpe?', 'answer': 'you can sign up for bharatpe by verifying your mobile number and then completing your kyc using pan and aadhaar number or any other information as required by the rbi registered lender'}, {'question': 'what is the maximum credit limit i can get?', 'answer': 'you can get a credit limit of up to 210 lakhs post completion of full kyc and meeting the required criteria as maybe determined by rbi registered lenders. this limit may increase if you continue using bharatpe and making timely repayments.'}]\n"
     ]
    }
   ],
   "source": [
    "#  Extract the FAQ list from the dictionary\n",
    "faqs = faqs_dict.get(\"faqs\", [])  # Safely get the list\n",
    "\n",
    "#  Check if it's now a list\n",
    "print(\"Type of faqs after extraction:\", type(faqs))  # Should be <class 'list'>\n",
    "print(faqs[:3])  # Print first 3 FAQs to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Tokenize all questions and answers\n",
    "tokenized_faqs = []\n",
    "for faq in faqs:\n",
    "    tokens = tokenizer(faq[\"question\"] + \" \" + faq[\"answer\"], return_tensors=\"pt\")\n",
    "    tokenized_faqs.append(tokens.input_ids.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenized dataset\n",
    "with open(\"tokenized_faqs.json\", \"w\") as f:\n",
    "    json.dump(tokenized_faqs, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VECTOR EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load the pre-trained embedding model\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # Change to OpenAI model if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load BPE Tokenized Dataset\n",
    "file_path = r\"C:\\Users\\gvais\\OneDrive\\Desktop\\faq\\bharatpe_faqs.json\"  # Use full path\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    faq_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Convert Text to Vector Embeddings\n",
    "def generate_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Generate Embeddings for Each FAQ\n",
    "embeddings = []\n",
    "for item in faq_data[\"faqs\"]:\n",
    "    vector = generate_embedding(item[\"question\"] + \" \" + item[\"answer\"])\n",
    "    embeddings.append({\"question\": item[\"question\"], \"embedding\": vector})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Embeddings saved at: C:\\Users\\gvais\\OneDrive\\Desktop\\faq\\faq_embeddings.json\n"
     ]
    }
   ],
   "source": [
    "#  Save Embeddings to a JSON File\n",
    "embedding_file = r\"C:\\Users\\gvais\\OneDrive\\Desktop\\faq\\faq_embeddings.json\"\n",
    "with open(embedding_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(embeddings, f, indent=4)\n",
    "\n",
    "print(f\" Embeddings saved at: {embedding_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate & Store Embeddings using Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ollama\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load the FAQ dataset\n",
    "file_path = r\"C:\\Users\\gvais\\OneDrive\\Desktop\\faq\\bharatpe_faqs.json\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    faq_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Initialize ChromaDB for Storing Embeddings\n",
    "chroma_client = chromadb.PersistentClient(path=r\"C:\\Users\\gvais\\OneDrive\\Desktop\\faq\\chroma_db\")\n",
    "collection = chroma_client.get_or_create_collection(name=\"faq_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ran for 1+ hr and still running so commemted it out for now\n",
    "# #  Generate and Store Embeddings\n",
    "# for item in faq_data[\"faqs\"]:\n",
    "#     text = item[\"question\"] + \" \" + item[\"answer\"]\n",
    "#     response = ollama.embeddings(model=\"mistral\", prompt=text)  # Use your preferred LLaMA model\n",
    "#     embedding = response[\"embedding\"]\n",
    "\n",
    "#     # Store in ChromaDB\n",
    "#     collection.add(\n",
    "#         ids=[item[\"question\"]],  # Use question as unique ID\n",
    "#         embeddings=[embedding],\n",
    "#         metadatas=[{\"question\": item[\"question\"], \"answer\": item[\"answer\"]}]\n",
    "#     )\n",
    "\n",
    "# print(\" Embeddings successfully generated and stored in ChromaDB!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Our Tokenized Dataset into QLoRA-Compatible JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Paths for input and output files\n",
    "bpe_tokenized_file = r\"C:\\Users\\gvais\\OneDrive\\Desktop\\faq\\bharatpe_faqs.json\"  # BPE tokenized dataset\n",
    "output_jsonl_file = r\"C:\\Users\\gvais\\OneDrive\\Desktop\\faq\\bharatpe_qlora.jsonl\"  # QLoRA fine-tuning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load the tokenized FAQ dataset\n",
    "with open(bpe_tokenized_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    faq_data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " QLoRA dataset successfully saved at: C:\\Users\\gvais\\OneDrive\\Desktop\\faq\\bharatpe_qlora.jsonl\n"
     ]
    }
   ],
   "source": [
    "#  Convert the dataset to QLoRA format\n",
    "with open(output_jsonl_file, \"w\", encoding=\"utf-8\") as jsonl_file:\n",
    "    for item in faq_data[\"faqs\"]:\n",
    "        jsonl_entry = {\n",
    "            \"instruction\": item[\"question\"],  # Question from FAQ dataset\n",
    "            \"input\": \"\",  # No additional input for now\n",
    "            \"output\": item[\"answer\"]  # Answer from FAQ dataset\n",
    "        }\n",
    "        jsonl_file.write(json.dumps(jsonl_entry) + \"\\n\")  # Write in JSONL format\n",
    "\n",
    "print(f\" QLoRA dataset successfully saved at: {output_jsonl_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Split into Training & Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load formatted dataset\n",
    "with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Split into 90% training & 10% validation\n",
    "train_data, val_data = train_test_split(data, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train-Test split done!\n"
     ]
    }
   ],
   "source": [
    "#  Save splits\n",
    "with open(\"train.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(train_data)\n",
    "\n",
    "with open(\"val.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(val_data)\n",
    "\n",
    "print(\" Train-Test split done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Model with LoRA (QLoRA for Efficient Fine-Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "#  Load tokenizer & model\n",
    "model_name = \"facebook/opt-1.3b\"  # You can also use \"mistralai/Mistral-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 173 examples [00:00, 3737.53 examples/s]\n",
      "Generating validation split: 20 examples [00:00, 1077.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load training dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": \"train.jsonl\", \"validation\": \"val.jsonl\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Define training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_model\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
